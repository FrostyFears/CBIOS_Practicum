{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, r2_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, lasso_path\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original file may or may not be available however below is the code for reading the original file and building a dictionary with only the S. pneumoniae organism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into python (The path will change depending on the location of the file)\n",
    "Data = pd.read_csv(\"/Users/keenananderson-fears/Desktop/Dr. Liu Lab/Current AMR Project/NCBI_Python_Readable.csv\", \n",
    "                  sep='\\t',\n",
    "                  header=None)\n",
    "\n",
    "# First I create an empty list for the names of each object, an empty dataframe for the data to be stored and an \n",
    "# empty dictionary to append everything to using the names as my keys\n",
    "List_of_Names = []\n",
    "NCBI_Data = {}\n",
    "\n",
    "# I next create the outer loop for generating all of the bacteria/antibiotic combinations from each row of my \n",
    "# original matrix\n",
    "for row in Data.iterrows():\n",
    "    # I then begin by selecting a single row from the original matrix to parse through with my loop, turn it into a \n",
    "    # dataframe with a single column and remove the NaN values from the dataset\n",
    "    Data4 = Data.iloc[row[0],]\n",
    "    Data5 = pd.DataFrame(Data4)\n",
    "    Data6= Data5.dropna()\n",
    "    # Next I set up some dummy variables, lists & data frames, which I can append the data to from each element of \n",
    "    # my original matrix\n",
    "    DataFrame = pd.DataFrame()\n",
    "    Index = []\n",
    "    Column_List = pd.DataFrame()\n",
    "    Data_List = []\n",
    "    # Next I set my loop up to go through the data by row\n",
    "    for i in Data6.iterrows():\n",
    "        # I next have to get the data into dataframes which XGBoost is capable of handling\n",
    "        str = ','.join(i[1]) # Since it is a string I combine all strings at all commas\n",
    "        # I then split them and compute the begining/end of the features set and set this to an array, removing the \n",
    "        # third element which corresponds to the individual code\n",
    "        Column_List = np.delete(np.array(str.split(',')[int(len(str.split(','))/2):]), 2)\n",
    "        # I then split them and compute the begining/end of the feature values set and set this to an array, \n",
    "        #removing the third element which corresponds to the individual code\n",
    "        Data_List.append([np.delete(np.array(str.split(',')[0:int(len(str.split(','))/2)]), 2)])\n",
    "        # Next I isolate those elements which correspond to the individual codes and save those as my Index values\n",
    "        I = str.split(',')[2]\n",
    "        I = I.replace(\"(\",\"\").replace(\")\",\"\").replace(\" \",\"\").replace(\",\",\"\")\n",
    "        Index.append([I])\n",
    "        #Index.append([str.replace(\"(\",\"\").replace(\")\",\"\").replace(\" \",\"\").replace(\",\",\"\").split(',')[2]])\n",
    "    # Once we have all our information stored we can move back to the outer loop to begin saving the information\n",
    "    # into their respective dataframe objects\n",
    "    Data6.columns = [\"All\"] # I have to rename the column list so as to call it later when making the dataframe\n",
    "    Name = Data6[\"All\"].iloc[0].split(',')[0] # I want the output of this command to be the name of my dataframe \n",
    "                                              # below\n",
    "    Name = Name.replace(\" \", \"_\").replace(\":\",\"_\") # In order to the save the bacteria/antibiotic combinations as\n",
    "                                                   # a file name I need to replace the spaces and colons with \"_\"\n",
    "    # I convert my Index and Data to a dataframe to make it readable by the index parameter within the DataFrame \n",
    "    # function    \n",
    "    Index = pd.DataFrame(Index)\n",
    "    Data_List = pd.DataFrame(Data_List)\n",
    "    Data_List.columns = [\"All\"]\n",
    "    List_of_Names.append(Name)\n",
    "    # Lastly I set up the DataFrame and save this to my dictionary with the bacteria/antibiotic name as the key \n",
    "    # using only the Streptococcus_pneumoniae organism\n",
    "    DataFrame = pd.DataFrame(Data_List['All'].to_list(), columns = Column_List, index = Index)\n",
    "    if Name == \"Streptococcus_pneumoniae_cefuroxime\":\n",
    "        NCBI_Data[Name] = DataFrame\n",
    "    elif Name == \"Streptococcus_pneumoniae_clindamycin\":\n",
    "        NCBI_Data[Name] = DataFrame\n",
    "    elif Name == \"Streptococcus_pneumoniae_erythromycin\":\n",
    "        NCBI_Data[Name] = DataFrame\n",
    "    elif Name == \"Streptococcus_pneumoniae_meropenem\":\n",
    "        NCBI_Data[Name] = DataFrame\n",
    "    elif Name == \"Streptococcus_pneumoniae_tetracycline\":\n",
    "        NCBI_Data[Name] = DataFrame\n",
    "    elif Name == \"Streptococcus_pneumoniae_trimethoprim-sulfamethoxazole\":\n",
    "        NCBI_Data[Name] = DataFrame\n",
    "    #NCBI_Data[Name] = DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a pickle file of the dictionary to store in our working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data (serialize)\n",
    "with open('NCBI_Data.pickle', 'wb') as handle:\n",
    "    pickle.dump(NCBI_Data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (deserialize)\n",
    "with open('NCBI_Data.pickle', 'rb') as handle:\n",
    "    NCBI_Data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then concatenate our various dataframes within our dictionary based upon the union of all columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframes for each resistance for Strep\n",
    "df = pd.DataFrame()\n",
    "cfrx = NCBI_Data['Streptococcus_pneumoniae_cefuroxime']\n",
    "cnm = NCBI_Data['Streptococcus_pneumoniae_clindamycin']\n",
    "e = NCBI_Data['Streptococcus_pneumoniae_erythromycin']\n",
    "m = NCBI_Data['Streptococcus_pneumoniae_meropenem']\n",
    "t = NCBI_Data['Streptococcus_pneumoniae_tetracycline']\n",
    "ts = NCBI_Data['Streptococcus_pneumoniae_trimethoprim-sulfamethoxazole']\n",
    " \n",
    "#Concatenate from above dataframes\n",
    "df = pd.concat([cfrx,cnm,e,m,t,ts], axis=0, ignore_index=True, sort=True)\n",
    " \n",
    "#Remove columns with NAs and call df 'shared_variants'\n",
    "shared_variants = df.dropna(axis=1, how='any', thresh=None, subset=None, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataframe now has a column called Names with the organism and antibiotic treatment. This being a catagorical vector cannot be read directly by any statistical software and must thus be encoded. Therefore we use a on-hot-encoding method to avoid any ordinal covariates within our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "data = shared_variants.iloc[:,-1:]\n",
    "values = array(data)\n",
    "values = values.flatten()\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "OHEData = pd.DataFrame(data=onehot_encoded, columns=[\"Streptococcus_pneumoniae_cefuroxime\",\n",
    "                                                    \"Streptococcus_pneumoniae_clindamycin\",\n",
    "                                                    \"Streptococcus_pneumoniae_erythromycin\",\n",
    "                                                    \"Streptococcus_pneumoniae_meropenem\",\n",
    "                                                    \"Streptococcus_pneumoniae_tetracycline\",\n",
    "                                                    \"Streptococcus_pneumoniae_trimethoprim-sulfamethoxazole\"])\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then take our encoded features, add them to our dataframe and remove the name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = shared_variants.iloc[:,:-1]\n",
    "Final_Data = pd.concat([test, OHEData], axis=1).reindex(shared_variants.index)\n",
    "Final_Data.to_csv('CBIOS_Full_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split our data into a testing set and training set for screening and further feature selection. Using the N/log10(N) cutoff for feature screening, where N is the number of columns, we find that our ideal number of features is around 344. Following this we can use a lasso path to identify the top 344 features in terms of their effect on our Y's, AMR, variance. Feature screening is performed using glmnet in R (see CBIOS_Project.Rmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
